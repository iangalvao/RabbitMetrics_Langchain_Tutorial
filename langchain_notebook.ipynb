{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e28ad4bc-9425-4701-be66-828c75ae2a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26/708083734.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv,find_dotenv\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pinecone\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e3182ab-7225-41a6-96ea-a9ccb2fb00ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64064ff3-0413-4d5e-8d9c-b61a0ac8e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(model_name=\"text-davinci-003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "524d3ccf-6790-46ed-91bf-744e86e8522d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nLarge language models are neural networks that are trained on large amounts of text data to generate human-like language.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"explain large language models in one sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b7620d-0dd8-4b56-87ee-afb64304b6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2028889-c4aa-49fb-b715-08e1fc2fccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an expert data scientist\"),\n",
    "    HumanMessage(content=\"Write a python program that trains a neural network on simulated data\")\n",
    "]\n",
    "response = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "504e1eba-bbcc-4178-94fb-8a684f5ab736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's an example of a Python program that trains a neural network on simulated data using the Keras library:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "\n",
      "# Generate simulated data\n",
      "np.random.seed(0)\n",
      "X = np.random.rand(1000, 2)\n",
      "y = np.random.randint(2, size=(1000, 1))\n",
      "\n",
      "# Define the neural network architecture\n",
      "model = Sequential()\n",
      "model.add(Dense(10, input_dim=2, activation='relu'))\n",
      "model.add(Dense(1, activation='sigmoid'))\n",
      "\n",
      "# Compile the model\n",
      "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "\n",
      "# Train the model\n",
      "model.fit(X, y, epochs=10, batch_size=32)\n",
      "\n",
      "# Evaluate the model\n",
      "loss, accuracy = model.evaluate(X, y)\n",
      "print(f'Loss: {loss:.4f}')\n",
      "print(f'Accuracy: {accuracy:.4f}')\n",
      "```\n",
      "\n",
      "In this example, we first generate simulated data using `numpy.random.rand` and `numpy.random.randint`. We create a neural network model using the `Sequential` class from Keras and add two fully connected layers using the `Dense` class. The first layer has 10 units and uses the ReLU activation function, while the second layer has 1 unit and uses the sigmoid activation function.\n",
      "\n",
      "We then compile the model using the binary cross-entropy loss function and the Adam optimizer. After that, we train the model using the `fit` method, specifying the number of epochs and the batch size.\n",
      "\n",
      "Finally, we evaluate the trained model on the same simulated data and print the loss and accuracy./n"
     ]
    }
   ],
   "source": [
    "print(response.content,end=\"/n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "315fbab3-8873-4af6-bd36-1cfb55fbc141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert data scientist with an expertise in building deep learning models.\n",
    "Explain the concept of {concept} in a couple of lines\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"concept\"],\n",
    "    template=template\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5beb79ed-0af8-4394-9e94-aa7377287b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['concept'], template='\\nYou are an expert data scientist with an expertise in building deep learning models.\\nExplain the concept of {concept} in a couple of lines\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81b0a08c-6d86-4548-8020-83d762a90124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRegularization is a technique used to avoid overfitting, which is when a model performs well on training data but poorly on unseen data. It is done by adding a penalty to the cost function of a model, such as the weights of the model, to reduce the complexity of the model and to prevent overfitting.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt.format(concept=\"regularization\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27aaf99b-d506-43e2-8dda-c0bf42d4b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c909fe29-27d1-47ef-bdfa-3562077c2468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'concept': 'autoencoder', 'text': \"\\nAn autoencoder is an unsupervised learning algorithm that takes an input, compresses it into a lower dimensional space (the 'encoding'), and then reconstructs the original input from the encoding. It is used for data compression, feature extraction, and dimensionality reduction.\"}\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "print(chain(\"autoencoder\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c22ddb5-87a0-463a-b22e-a19988bbb982",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt=PromptTemplate(\n",
    "    input_variables=['ml_concept'],\n",
    "    template=\"Turn the concept description of {ml_concept} and explain it to me like I'm five in 500 words\",\n",
    ")\n",
    "chain_two = LLMChain (llm=llm, prompt=second_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a248a48d-b2d6-4eb9-ba7a-b7aa7c8625f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "Autoencoders are a type of neural network that can be used to learn efficient data representations in an unsupervised manner. They are composed of an encoder that maps the input data into a low-dimensional representation, and a decoder that reconstructs the input data from the low-dimensional representation.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "Autoencoders are a type of fun tool that computers use to learn about different kinds of data. It's like when you have a bunch of toys that you can't play with, so you have to figure out what they are and how you can use them. \n",
      "\n",
      "Autoencoders can help computers learn about data without being given any instructions. They use two different parts that work together. The first part is called an encoder. It takes the raw data and turns it into something smaller and simpler. It's like taking a big, complicated toy and breaking it down into smaller pieces. \n",
      "\n",
      "The second part is called a decoder. It takes the smaller, simpler data from the encoder and turns it back into something bigger and more complicated. It's like taking the pieces of the toy and putting them back together to make something new. \n",
      "\n",
      "So, autoencoders help computers learn about data without being told what to do. They can take something complicated and make it simpler, and then take the simpler version and make it more complicated. This helps them learn better and faster.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "Autoencoders are a type of fun tool that computers use to learn about different kinds of data. It's like when you have a bunch of toys that you can't play with, so you have to figure out what they are and how you can use them. \n",
      "\n",
      "Autoencoders can help computers learn about data without being given any instructions. They use two different parts that work together. The first part is called an encoder. It takes the raw data and turns it into something smaller and simpler. It's like taking a big, complicated toy and breaking it down into smaller pieces. \n",
      "\n",
      "The second part is called a decoder. It takes the smaller, simpler data from the encoder and turns it back into something bigger and more complicated. It's like taking the pieces of the toy and putting them back together to make something new. \n",
      "\n",
      "So, autoencoders help computers learn about data without being told what to do. They can take something complicated and make it simpler, and then take the simpler version and make it more complicated. This helps them learn better and faster.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain,chain_two], verbose=True)\n",
    "\n",
    "explanation = overall_chain.run(\"autoencoder\")\n",
    "print (explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cefbfa08-5d34-4dc9-af33-8ead57012726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "HNSW (Hierarchical Navigable Small World) is an algorithm used for efficient approximate nearest neighbor search in high-dimensional data. It works by constructing a hierarchical graph of nodes connected by edges, which represent the similarity between pairs of points in the data set. The algorithm is fast and scalable, making it suitable for large-scale applications.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "HNSW is an algorithm that helps you find things that are close to each other. Imagine you have a bunch of apples in a box. You want to find the apple that is the closest to you. You can reach in and grab one, but that might not be the closest. HNSW helps you find the closest apple in the box.\n",
      "\n",
      "HNSW works by creating a graph of all the apples in the box. This graph is like a map of all the apples. Each apple is connected to other apples by edges. The edges represent how similar the apples are. So if two apples are very similar, they will have a strong connection on the graph. \n",
      "\n",
      "The algorithm then looks at the graph to figure out which apple is closest to you. It starts at your apple and then looks at the edges to see which other apples it is connected to. It then looks at those apples and their edges, and so on. This process is repeated until it finds an apple that is closest to you.\n",
      "\n",
      "HNSW is fast and efficient, which means it can work for large boxes with lots of apples. This makes it great for big data sets, which means it can be used for lots of different things, like finding similar\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "HNSW is an algorithm that helps you find things that are close to each other. Imagine you have a bunch of apples in a box. You want to find the apple that is the closest to you. You can reach in and grab one, but that might not be the closest. HNSW helps you find the closest apple in the box.\n",
      "\n",
      "HNSW works by creating a graph of all the apples in the box. This graph is like a map of all the apples. Each apple is connected to other apples by edges. The edges represent how similar the apples are. So if two apples are very similar, they will have a strong connection on the graph. \n",
      "\n",
      "The algorithm then looks at the graph to figure out which apple is closest to you. It starts at your apple and then looks at the edges to see which other apples it is connected to. It then looks at those apples and their edges, and so on. This process is repeated until it finds an apple that is closest to you.\n",
      "\n",
      "HNSW is fast and efficient, which means it can work for large boxes with lots of apples. This makes it great for big data sets, which means it can be used for lots of different things, like finding similar\n"
     ]
    }
   ],
   "source": [
    "explanation = overall_chain.run(\"HNSW algorithm\")\n",
    "print (explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "615e53df-fc1c-4529-8ac7-c028bd7f2d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1b23dfd-8757-4bca-b5ba-6cc938e15f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap = 0\n",
    ")\n",
    "texts = text_splitter.create_documents([explanation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7424b72e-cc16-4611-bcf6-cd7764fde8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Autoencoders are a type of fun tool that computers use to learn about different kinds of data. It's\"),\n",
       " Document(page_content=\"like when you have a bunch of toys that you can't play with, so you have to figure out what they\"),\n",
       " Document(page_content='are and how you can use them.'),\n",
       " Document(page_content='Autoencoders can help computers learn about data without being given any instructions. They use two'),\n",
       " Document(page_content='different parts that work together. The first part is called an encoder. It takes the raw data and'),\n",
       " Document(page_content=\"turns it into something smaller and simpler. It's like taking a big, complicated toy and breaking\"),\n",
       " Document(page_content='it down into smaller pieces.'),\n",
       " Document(page_content='The second part is called a decoder. It takes the smaller, simpler data from the encoder and turns'),\n",
       " Document(page_content=\"it back into something bigger and more complicated. It's like taking the pieces of the toy and\"),\n",
       " Document(page_content='putting them back together to make something new.'),\n",
       " Document(page_content='So, autoencoders help computers learn about data without being told what to do. They can take'),\n",
       " Document(page_content='something complicated and make it simpler, and then take the simpler version and make it more'),\n",
       " Document(page_content='complicated. This helps them learn better and faster.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fb7ea33-0f75-4e63-9f7d-b411d74766f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Autoencoders are a type of fun tool that computers use to learn about different kinds of data. It's\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91b3f5ed-213b-4ebf-baaa-7dd4fb9e1dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(deployment=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b08cbb85-0220-4114-b7a7-15b237b58969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base='', openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization='', allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=6, request_timeout=None, headers=None, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "484a8d77-209b-4811-bd05-f84092488aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "import os\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "578206ef-5cf4-42a5-81a6-a793ca0efbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(\n",
    "    api_key=os.getenv('PINECONE_API_KEY'),\n",
    "    environment = os.getenv('PINECONE_ENV')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37197b94-1bf1-4458-85c7-dbe81d7e714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"langchain-tutorial\"\n",
    "search = Pinecone.from_documents(texts, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf936f3b-1256-42f2-b94e-a02bdae8598a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.324'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e13fb63-60c2-44dc-b09a-3f9616b781ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is fun about an autoencoder?\"\n",
    "search_results =  search.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7f22f4e-df5f-4149-bb43-dbc6ba2ae16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"Autoencoders are a type of fun tool that computers use to learn about different kinds of data. It's\"), Document(page_content='Autoencoders can help computers learn about data without being given any instructions. They use two'), Document(page_content='So, autoencoders help computers learn about data without being told what to do. They can take'), Document(page_content='complicated. This helps them learn better and faster.')]\n"
     ]
    }
   ],
   "source": [
    "print(search_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
